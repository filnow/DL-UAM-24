{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from types import FunctionType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, n_inputs, learning_rate=0.1):\n",
    "        self.weights = np.zeros(n_inputs)\n",
    "        self.bias = 0\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        return np.heaviside(self.weights @ inputs + self.bias, 0)\n",
    "    \n",
    "    def backward(self, inputs, error):\n",
    "        self.weights += self.learning_rate * error * inputs\n",
    "        self.bias += self.learning_rate * error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(perceptron: Perceptron, \n",
    "                     inputs: np.ndarray, outputs: np.ndarray) -> None:\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        pred = perceptron(input)\n",
    "        error = output - pred    \n",
    "        perceptron.backward(input, error)\n",
    "\n",
    "def test_perceptron(perceptron: Perceptron, inputs: \n",
    "                    np.ndarray, outputs: np.ndarray) -> None:\n",
    "    for input, output in zip(inputs, outputs):\n",
    "        pred = perceptron(input)\n",
    "        print(f\"{input} -> {output}\\nPredicted -> {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.1\n",
      "[2 3] -> 1\n",
      "Predicted -> 1.0\n",
      "[-1  2] -> 0\n",
      "Predicted -> 0.0\n",
      "\n",
      "Learning rate: 0.01\n",
      "[2 3] -> 1\n",
      "Predicted -> 1.0\n",
      "[-1  2] -> 0\n",
      "Predicted -> 0.0\n",
      "\n",
      "Learning rate: 0.001\n",
      "[2 3] -> 1\n",
      "Predicted -> 1.0\n",
      "[-1  2] -> 0\n",
      "Predicted -> 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = np.array([[2, 3], [-1, 2]])\n",
    "outputs = np.array([1, 0])\n",
    "\n",
    "learning_rate = [0.1, 0.01, 0.001]\n",
    "\n",
    "for lr in learning_rate:\n",
    "    perceptron = Perceptron(inputs.shape[1], learning_rate=lr)\n",
    "    print(f\"Learning rate: {lr}\")\n",
    "    train_perceptron(perceptron, inputs, outputs)\n",
    "    test_perceptron(perceptron, inputs, outputs)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, n_inputs: np.ndarray):\n",
    "        self.weights = np.random.uniform(-0.1, 0.1, size=n_inputs)\n",
    "        self.bias = np.random.uniform(-0.1, 0.1)\n",
    "    \n",
    "    def __call__(self, inputs: np.ndarray) -> np.ndarray:\n",
    "        return inputs @ self.weights + self.bias\n",
    "    \n",
    "    def backward(self, inputs: np.ndarray, \n",
    "                 error: np.ndarray,\n",
    "                 learning_rate: float) -> None:\n",
    "        dC_dw = inputs.T @ error\n",
    "        dC_db = np.sum(error)\n",
    "        self.weights -= learning_rate * dC_dw\n",
    "        self.bias -= learning_rate * dC_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def tanh(x):\n",
    "    return (2 / (1 + np.exp(-2 * x))) - 1\n",
    "\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * np.power(x, 3))))\n",
    "\n",
    "def swish(x):\n",
    "    return x * sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(error: float) -> float:\n",
    "    return np.mean(error**2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: Neuron, inputs: np.ndarray, \n",
    "          outputs: np.ndarray, epochs: int=100, \n",
    "          lr: float=0.1, activation: FunctionType=relu) -> None:\n",
    "\n",
    "    avg_loss = []\n",
    "    for _ in range(epochs):\n",
    "        loss = 0\n",
    "        pred = activation(model(inputs))\n",
    "        error = pred - outputs\n",
    "        model.backward(inputs, error, lr)\n",
    "        loss += mean_squared_error(error)\n",
    "        avg_loss.append(loss)\n",
    "\n",
    "    return round(sum(avg_loss)/epochs, 4), round(avg_loss[-1], 4)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: Neuron, inputs: np.ndarray, \n",
    "            activation: FunctionType) -> None:\n",
    "    return np.round(activation(model(inputs)), 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg loss -> 0.00350\n",
      "\n",
      "Final loss -> 0.00000\n",
      "\n",
      "[0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 1000\n",
    "LEARNING_RATE = 0.1\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "y_and = np.array([0, 0, 0, 1])\n",
    "y_or = np.array([0, 1, 1, 1])\n",
    "\n",
    "model = Neuron(X.shape[1])\n",
    "\n",
    "avg_loss, final_loss = train(model, X, y_and, epochs=EPOCH, lr=LEARNING_RATE, activation=relu)\n",
    "\n",
    "print(f\"Avg loss -> {avg_loss:.5f}\\n\")\n",
    "print(f\"Final loss -> {final_loss:.5f}\\n\")\n",
    "print(predict(model, X, activation=relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_csv(data, filename):\n",
    "    cols = ['Epoch', 'Input', 'Output', 'Predicted', 'Activation', 'Learning Rate', 'Avg Loss', 'Final Loss']\n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "    df.to_csv(filename, index=False, columns=cols, header=cols, sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiments(operator: str):\n",
    "\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "    y_and = np.array([0, 0, 0, 1])\n",
    "    y_or = np.array([0, 1, 1, 1])\n",
    "\n",
    "    activation_functions = [sigmoid, relu, tanh, gelu, swish]\n",
    "    epochs = [1, 10, 50, 100, 1000]\n",
    "    learning_rate = [0.1, 0.01, 0.001]\n",
    "    results = []\n",
    "\n",
    "    for i in activation_functions:\n",
    "        for j in epochs:\n",
    "            for lr in learning_rate:\n",
    "                model = Neuron(X.shape[1])\n",
    "                y = y_and if operator == 'and' else y_or\n",
    "                avg_loss, final_loss = train(model, X, y, epochs=j, lr=lr, activation=i)\n",
    "                results.append([j, X.tolist(), y.tolist(), predict(model, X, activation=i), i.__name__, lr, avg_loss, final_loss])\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_and = experiments('and')\n",
    "results_or = experiments('or')\n",
    "\n",
    "create_csv(results_and, 'and.csv')\n",
    "create_csv(results_or, 'or.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wnioski\n",
    "\n",
    "Dla operatora AND najlepsze rezultaty osiąga funkcja aktywacji ReLU, jest ona jednocześnie najprostszą i dzięki temu najbardziej optymalną wydajnościowo funckją. Najlpeszy Learning Rate wynosi 0.1, przy mniejszych wartościach jak 0.01 lub 0.001, neuron uczy się zdecydowanie wolniej przy 0.01 jest to nie wielka różnica, lecz wzrasta ona zdecydowanie przy 0.001, jak można zauważyć w excelu z wynikami. Opowiednią ilość Epoch dla tej bramki to 1000, co ciekawe można zauważyc, że przy 100 epochcach neuron z learning rate 0.1 radzi sobie bardzo podobnie jak ten z 1000 przy learning rate 0.01. Dzieląc learning rate przez 10 musi pomnożyć wartość epoch o 10 by uzyskać podobny wynik. \n",
    "\n",
    "Dla operatora OR najlepiej radzi sobie funkcja aktywacji Tanh przy takim samym learning rate i wartości epoch jak dla bramki AND.\n",
    "\n",
    "Neuron potrafi także sprawnie nauczyć się bramek AND i OR przy znacznie mniejszej ilości epoch, przykładowo dla bramek OR z funkcja aktywacji tanh i 10 epochami otrzymujemy wyniki [0.5, 0.81, 0.82, 0.94], jest to bardzo bliskie bycia trafną predykcją zakładając, że użyjemy do klasyfikacji funkcji Heaviside(0 gdy x <= 0.5, inaczej 1), wartości dla etykiety 0 są jednak bardzo blisko granicy, co w przypadku testowania na inncyh danych może prowadzić do błędnej klasyfikacji i dużej ilości etykietowania jako false positive, czyli oznaczania 0 jako 1, dlatego lepiej wydłużyć czas uczenia się do przynajmniej 100 epoch dla uzyskania odpowiednich rezultatów, najlepiej by było to 1000 epoch, lecz wtedy trzebą uwazać na tak zwany overfitting, tak by nasz neuron nie zaczął zapamietywać danych uczących.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
